---
title: "STAND - Stress and Anxiety Detection and Level Estimation"
collection: projects
link: https://aliknd.github.io/projects/2023-01-15-project-number-6
excerpt: 'Based on an EMA based intervention, this study intends to use all modalities to efficiently recognize stress, a commonly experienced emotional state. The study will involve collecting data (visual, speech, text, and time series) from participants for a month and then analyzing the data to identify the key features associated with accurately recognizing stress and anxiety.'
date: 2023-01-15
venue: 'both Android and iOS'
#paperurl: 'https://aliknd.github.io/files/emotionwild.pdf'
#citation: 'Qian, Y., Kargarandehkordi, A., Mutlu, O. C., Surabhi, S., Honarmand, M., Wall, D. P., & Washington, P. (2023). Computer Vision Estimation of Emotion Reaction Intensity in the Wild. arXiv preprint arXiv:2303.10741.'
---

STAND (STress and ANxiety Detection) is a multimodal digital phenotyping project aimed at developing AI systems capable of detecting and quantifying stress and anxiety in real time. The project integrates smartphone-based video, game-based behavioral signals, ecological momentary assessments (EMA), and wearable biosensing to build rich, individualized profiles of stress and affective dynamics.

The STAND mobile app engages participants in a set of cognitively demanding and stress-inducing games while simultaneously capturing frontal-face video and behavioral performance metrics (e.g., reaction time, accuracy, hesitation patterns). These recordings serve as training data for computer-vision and multimodal learning models that estimate moment-level stress and anxiety levels.

To expand the ecological validity and sensing depth, the newest phase of STAND also incorporates wearable devices during gameplay to collect heart rate, HRV, EDA, and movement signals. This enables the creation of multimodal datasets that fuse physiological, behavioral, and visual markers of stress—supporting the development of robust, real-world affective-detection models.

Current development focuses on enhancing the app’s gameplay experience, refining data capture workflows, and expanding engagement features, including:

- Automatic upload of all captured video, sensor data, and gameplay logs to secured AWS storage
- Gamification elements such as daily rewards, streak counters, and a weekly leaderboard to encourage sustained participation
- Integration of a second stress-inducing game to broaden the types of cognitive and emotional challenges presented
- Synchronization of wearable biosignals to enrich multimodal model training for stress and anxiety detection

The STAND platform is actively evolving toward a scalable multimodal dataset and serves as the foundation for my upcoming work on personalized affective modeling during my UCSF postdoctoral fellowship.

You can find the code for this app in <a href="https://github.com/aliknd/STAND_Digital_Health_App">this public repository</a> on Github.

![0013](https://github.com/aliknd/aliknd.github.io/assets/96740009/6000895c-45e6-46db-9d46-008e734f3922)


![0014](https://github.com/aliknd/aliknd.github.io/assets/96740009/48048683-0e7c-4aa7-b9b8-5ca447dc17b3)


![workflow---Copy](https://github.com/aliknd/aliknd.github.io/assets/96740009/e237e011-ae95-4868-807a-e70b83835336)

