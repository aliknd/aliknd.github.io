---
title: "STAND - Stress and Anxiety Detection and Estimation Level"
collection: projects
link: https://aliknd.github.io/projects/2023-01-15-project-number-6
excerpt: 'Based on an EMA based intervention, this study intends to use all modalities to efficiently recognize stress, a commonly experienced emotional state. The study will involve collecting data (visual, speech, text, and time series) from participants for a month and then analyzing the data to identify the key features associated with accurately recognizing stress and anxiety.'
date: 2023-01-15
venue: 'both Android and iOS'
#paperurl: 'https://aliknd.github.io/files/emotionwild.pdf'
#citation: 'Qian, Y., Kargarandehkordi, A., Mutlu, O. C., Surabhi, S., Honarmand, M., Wall, D. P., & Washington, P. (2023). Computer Vision Estimation of Emotion Reaction Intensity in the Wild. arXiv preprint arXiv:2303.10741.'
---

Emotions play an essential role in human communication. Developing computer vision models for automatic recognition of emotion expression can aid in a variety of domains, including robotics, digital behavioral healthcare, and media analytics. There are three types of emotional representations which are traditionally modeled in affective computing research: Action Units, Valence Arousal (VA), and Categorical Emotions. As part of an effort to move beyond these representations towards more fine-grained labels, we describe our submission to the newly introduced Emotional Reaction Intensity (ERI) Estimation challenge in the 5th competition for Affective Behavior Analysis in-the-Wild (ABAW). We developed four deep neural networks trained in the visual domain and a multimodal model trained with both visual and audio features to predict emotion reaction intensity. Our best performing model on the Hume-Reaction dataset achieved an average Pearson correlation coefficient of 0.4080 on the test set using a pre-trained ResNet50 model. This work provides a first step towards the development of production-grade models which predict emotion reaction intensities rather than discrete emotion categories.
